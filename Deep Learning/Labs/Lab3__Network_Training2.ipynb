{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/2020_04_Network_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network Training\n",
        "\n",
        "In this Lab, we will introduce different topics related to the training phase of deep neural networks,  including data augmentation, optimization/regularization techniques, weight initializations, loss function and hyperparameters tuning.\n"
      ],
      "metadata": {
        "colab_type": "text",
        "id": "91E7ozNTWvS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary\n",
        "\n",
        "Here, we load the MNIST dataset and define a model for digit classification by using our own method `get_data_model`. We also define a new function `plot_history`, which plots the training loss and if required the validation loss. "
      ],
      "metadata": {
        "colab_type": "text",
        "id": "hVa-ZTYsXhNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import np_utils\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "def plot_history(history, metric = None):\n",
        "  # Plots the loss history of training and validation (if existing)\n",
        "  # and a given metric\n",
        "  \n",
        "  if metric != None:\n",
        "    fig, axes = plt.subplots(2,1)\n",
        "    axes[0].plot(history.history[metric])\n",
        "    try:\n",
        "      axes[0].plot(history.history['val_'+metric])\n",
        "      axes[0].legend(['Train', 'Val'])\n",
        "    except:\n",
        "      pass\n",
        "    axes[0].set_title('{:s}'.format(metric))\n",
        "    axes[0].set_ylabel('{:s}'.format(metric))\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "    axes[1].plot(history.history['loss'])\n",
        "    try:\n",
        "      axes[1].plot(history.history['val_loss'])\n",
        "      axes[1].legend(['Train', 'Val'])\n",
        "    except:\n",
        "      pass\n",
        "    axes[1].set_title('Model Loss')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "  else:\n",
        "    plt.plot(history.history['loss'])\n",
        "    try:\n",
        "      plt.plot(history.history['val_loss'])\n",
        "      plt.legend(['Train', 'Val'])\n",
        "    except:\n",
        "      pass\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "  \n",
        "def get_data_model(args = {}):\n",
        "  # Returns simple model, flattened MNIST data and categorical labels\n",
        "  num_classes=10\n",
        "  \n",
        "  # the data, shuffled and split between train and test sets\n",
        "  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  x_train = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\n",
        "  x_test = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\n",
        "\n",
        "  x_train= x_train.astype('float32')\n",
        "  x_test= x_test.astype('float32')\n",
        "\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "\n",
        "  y_train=to_categorical(y_train,num_classes)\n",
        "  y_test=to_categorical(y_test,num_classes)\n",
        "\n",
        "  # Load simple model\n",
        "  model = Sequential()\n",
        "  model.add(Dense(512, activation='relu', input_shape=(784,), **args))\n",
        "  model.add(Dense(512, activation='relu', **args))\n",
        "  model.add(Dense(10, activation='softmax', **args))\n",
        "  return model, x_train, y_train, x_test, y_test\n",
        "\n",
        "model, x_train, y_train, x_test, y_test = get_data_model()\n",
        "model.summary()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "colab_type": "code",
        "id": "2aGBJBVmXd8b",
        "outputId": "f30d81b6-153b-4be2-ec0d-763b9d8d1ed4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizers\n",
        "\n",
        "Once the model has been created, it is necessary to define an optimizer. In a deep learning problem, optimizers take in each time-step the gradient of the loss function respect to the weights of the network. Then, the weights are updated to minimize the loss value. Different optimizers follow different rules for updating the weights. For example, the vanilla Stochastic Gradient Descent (SGD) only uses the gradient in the current time-step $t$ and the learning rate to update the weights. \n",
        "\n",
        "Since there are methods more prone to get stuck in local minima and other methods converging faster, the choice of the optimizer can potentially affect both the final performance and the speed of convergence.  A nice visualization of such behavior is the following animation, where you can notice some methods, i.e., Adadelta in yellow, and Rmsprop in black converge significantly faster than SGD in red failing in local minima. \n",
        "\n",
        "![](http://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif)\n",
        "\n",
        "The animation above is from [Sebastian Ruder's blog](http://ruder.io/optimizing-gradient-descent/), who wrote an interesting article about the math formulation and properties of different optimizers. In this tutorial, we will follow a hands-on approach and will mainly focus on how to use them in Keras. \n",
        "\n",
        "\n",
        "As a rule of thumb, Adam is usually easier to tune due to the adaptive learning rate, whereas SGD with momentum [has been shown](https://arxiv.org/pdf/1712.07628.pdf) to reach better results when tuned correctly. Consult the official documentation [Optimizers in Keras](https://keras.io/optimizers/) to see the available implemented optimizers in Keras.\n",
        "\n",
        "When defining a model in Keras we need to call the `compile` method, which  requires two arguments in string format: a loss function and an optimizer. For example, in the next piece of code we use as optimizer Adam, which is an widely used optimizer, and as our loss function the categorical cross-entropy."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "mRx68ymWLCT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
        "model.fit(x_train,y_train,batch_size=100, epochs=10, verbose=1,validation_data=(x_test,y_test))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "colab_type": "code",
        "id": "9od5lUWMWu5X",
        "outputId": "c760c495-80cd-408c-d569-90851799c46a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializers\n",
        "\n",
        "\n",
        "Neural Networks have a large number of weights that we need to initialize before starting training. Weight initialization is a crucial step in tuning neural networks as different initial values can lead the model to reach different local minima. The weights are usually randomly initialized by different algorithms, e.g. Xavier,  He_normal, LeCun. \n",
        "\n",
        "In Keras, you can set the particular initialization strategy you want to use as an argument when declaring a layer. For example, in the function `Dense()` which defines the mapping $y= Ax + b$, you can initialize the kernel values ($A$ in the equation) with a normal distribution (by default the `stddev` is 0.05) and the bias ($b$ in the equation) with $0$ using the code below."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "-yDSecajTyc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_layer = Dense(64, kernel_initializer='random_normal',\n",
        "                bias_initializer='zeros')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "A6lzySByXcMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's check the weights of the layers and see if they follow the distributions we set. "
      ],
      "metadata": {
        "colab_type": "text",
        "id": "oF_EVmyooqYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "input_x = K.variable(np.random.random((1, 64)))\n",
        "y = linear_layer(input_x)\n",
        "weights = linear_layer.get_weights()\n",
        "# Weights return an array with [kernel, bias]\n",
        "# Let's see the kernel weights\n",
        "print(weights[0])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "colab_type": "code",
        "id": "YgHC0CSmowIK",
        "outputId": "ddf0feab-1085-40ee-cb61-52df8ed8eed0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's check that the mean is 0 and stddev is 0.05\n",
        "print(weights[0].mean(), weights[0].std())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "kLDzNuqUp2M8",
        "outputId": "3cd952e0-7757-4d8e-a07a-6b8ed732303f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's print the bias now\n",
        "print(weights[1])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "Uohj6D4rqTfT",
        "outputId": "84138382-95db-4fb9-fdde-744566911c07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of initializations available in Keras is listed [in the documentation](https://keras.io/initializers/). By default in Keras the kernel weights are initialized as `'glorot_uniform'` and the bias to `'zeros'`. Glorot uniform, which is also called Xavier initialization was defined [here](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). It samples the weights from a uniform distribution, whose range depends on the number of input and output units. Another initializer quite used is the `he_normal`, which draws the weights from a truncated normal distribution."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "69NkDCipqbmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss\n",
        "\n",
        "Another important step in training deep neural networks is the choice of the loss function, which strictly depends on your problem. In this tutorial, we will introduce two typical losses i.e., cross entropy and mean squared error, for two standard problems in machine learning: classification and regression. The full list of standard losses in Keras is available [here](https://keras.io/losses/)."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "1-8ittW2L1DI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification\n",
        "\n",
        "For classification problems, the standard loss used is the cross-entropy loss. For the binary case, the formula is $\\mathcal{L} = y\\log(p) + (1-y)\\log(1-p)$, where $p$ is a probability value between $[0, 1]$.\n",
        "To constrain the activations to assume such values, typically a [Sigmoid activation](https://en.wikipedia.org/wiki/Sigmoid_function) is applied. In Keras, the presented binary classification loss is called `binary_crossentropy`, and it accepts as target a vector with an element in the range $[0, 1]$ (usually either $0$ or $1$) per input element."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "0T8aSdjCtSSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, x_train, y_train, x_test, y_test = get_data_model()\n",
        "model.pop()\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# This three lines transform the problem in a binary one\n",
        "# We want to know if the number is bigger than 5 (label 1) or smaller (label 0)\n",
        "y_train = np.argmax(y_train, axis = 1)\n",
        "y_train[y_train < 5] = 0\n",
        "y_train[y_train >= 5] = 1\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=3, batch_size=32,  validation_split=0.2, verbose = 0)\n",
        "plot_history(history, 'binary_accuracy')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "colab_type": "code",
        "id": "LUQhLNxr5VA7",
        "outputId": "d4d913ba-1fbb-4ae6-99ad-a6c243631b71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case the number of classes is higher than 2, we use the cross-entropy loss, which has the form of $\\mathcal{L} = -\\sum_i y_i\\log(p_i)$. The loss is called `categorical_crossentropy` in Keras, and accepts one-hot encoded vectors. A one-hot encoded vector has dimensionality $C$, where $C$ is the number of classes. All of the elements are set to 0, minus the corresponding class $c$, which is set to 1. If we have a vector of labels with a scalar from $[0, C)$ per training example, we can transform it into a one-hot encoding form by using the function `to_categorical`.  \n",
        "\n",
        "Let's see an example using MNIST data."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "6jvNHqg--XYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# The labels are an scalar from 0 to 9 per example\n",
        "print(y_train[:5])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "v_YBOLN77S1I",
        "outputId": "ea42b152-753c-45f2-f7f0-8645dd304ece"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.to_categorical(y_train[:5])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "colab_type": "code",
        "id": "Rgm-W7sa5U-P",
        "outputId": "a31e5b9e-5e73-4a06-88b7-77daac2c76da"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output vector needs to be $\\sum_i p_i = 1$, and we achieve that by applying the softmax activation function to the output vector."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "OR_lJgB6CfFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the function get_data_model, which already applies to_categorical\n",
        "_, x_train, y_train, x_test, y_test = get_data_model()\n",
        "\n",
        "### Model defined with softmax\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=3, batch_size=32,  validation_split=0.2, verbose = 0)\n",
        "plot_history(history, 'categorical_accuracy')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "colab_type": "code",
        "id": "BVTAPlc_B99R",
        "outputId": "5ee5e17d-d6f4-4586-8540-aeb50dd326cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression\n",
        "For regression problems it is quite standard to use as losses the Mean Squared Error (MSE) or Mean Absolute Error (MAE), depending on the problem. \n",
        "\n",
        "As in the evaluation measures exercise, we will load the Boston Housing dataset."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "iFuOCwNftQgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import boston_housing\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
        "print(y_train[:10])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "Li20u1NjrwS5",
        "outputId": "96c547e3-7354-4646-97d8-992f60199c41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the labels are float numbers, and our goal is to predict them. To do so, we need a network that has only one output. Now, we will train the network using MAE and MSE. "
      ],
      "metadata": {
        "colab_type": "text",
        "id": "0JO5_oiCyPzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_mae = Sequential()\n",
        "model_mae.add(Dense(100, activation='relu', input_shape=(13,)))\n",
        "model_mae.add(Dense(1))\n",
        "model_mae.compile(optimizer='adam',loss='mean_absolute_error', metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "history = model_mae.fit(x_train, y_train, epochs=100, batch_size=32,  validation_split=0.2, verbose = 0)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RSJH6_Mfsjp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history, 'mean_absolute_error')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "colab_type": "code",
        "id": "J2xZRys8zNsD",
        "outputId": "3df6370a-91a2-4ef2-feea-77d32ed09d34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_mse = Sequential()\n",
        "model_mse.add(Dense(100, activation='relu', input_shape=(13,)))\n",
        "model_mse.add(Dense(1))\n",
        "model_mse.compile(optimizer='adam',loss='mean_squared_error', metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "history = model_mse.fit(x_train, y_train, epochs=100, batch_size=32,  validation_split=0.2, verbose = 0)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2XYKtV1GzMQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history, 'mean_squared_error')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "colab_type": "code",
        "id": "SBZB_MDCw1Os",
        "outputId": "d3bc8b0f-410e-4508-9baa-f69e934bf44f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model_mae.evaluate(x_test, y_test)\n",
        "print('MAE trained model achieves MAE: {:.4f} and MSE: {:.4f}'.format(results[1], results[2]))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "aMHRntQU0mrM",
        "outputId": "c6e7bb99-4d07-4242-c47f-6092e82f8bc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model_mse.evaluate(x_test, y_test)\n",
        "print('MSE trained model achieves MAE: {:.4f} and MSE: {:.4f}'.format(results[1], results[2]))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "df2igCnI030F",
        "outputId": "cc6e9fbd-7ea5-4952-be9c-109f2cf1f85f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plots show that training using MSE as loss achieves a better MSE and worse MAE in the test set compared to the model training with MAE loss."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "hWr2tc7l0IdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's print some predicted prices from the test set, along with the actual price, just to have an intuition of the output values."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "vnEITU4iy6Q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_prices = model_mae.predict(x_test)\n",
        "print(np.around(pred_prices[:10, 0], 2))\n",
        "print(y_test[:10])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "uTPGyWHxydF_",
        "outputId": "b6dafdb8-fc0e-4bd1-db45-ee7d99e99371"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A problem usually treated as a regression problem with deep learning approaches is depth estimation, where the depth of an image is estimated using the RGB information. Some other regression losses are also used in some settings (e.g. Huber loss or Reverse Huber loss, or the already introduced Mean Percentage Absolute Error), but MAE and MSE are the most widely used. "
      ],
      "metadata": {
        "colab_type": "text",
        "id": "z_VeO_-ORHnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularization\n",
        "As mentioned in the lecture, regularization is an effective tool to fight some of the problems we may have during training, such as vanishing/exploding gradients or overfitting to the training set. We now mention some of the usual ways to regularize our training process. "
      ],
      "metadata": {
        "colab_type": "text",
        "id": "miKutjvgtXuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss regularizers\n",
        "\n",
        "[Regularizers](https://keras.io/regularizers/)  put some penalties to the optimization process. In practice by penalizing large values, weights are constrained to be small which can help us prevent overfitting.\n",
        "\n",
        "In Keras regularization works on a per-layer basis. It means you can define a regularization function for each layer. In particular,  you can specify three regularization parameters each one related to a different type of penalty:\n",
        "\n",
        "*   `kernel_regularizer`: a penalty depending on the value of the kernel weights, e.g, larger kernel weights result in larger penalization.\n",
        "*   `bias_regularizer`: a penalty depending on the loss function is applied to the value of the bias. If you want the output function to pass through (or have an intercept closer to) the origin, you can use the `bias_regularizer`.\n",
        "*   `activity_regularizer`: a penalty applied to the loss function depending on the layer's output. It results in smaller outputs in value when this regularizer is applied.\n",
        "\n",
        "Standard regularizers that can be applied are $l_1$, $l_2$ and $l_1+l_2$. \n",
        "In the example below, we check the difference in training and validation accuracy by varying the used regularization strategy.\n"
      ],
      "metadata": {
        "colab_type": "text",
        "id": "F-VrqhSbWJ9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import regularizers\n",
        "test_accuracy = []\n",
        "train_accuracy = []\n",
        "reg_values = [0.001, 0.0003, 0.0001, 0.00003, 0.00001, 0.000003, 0.000001]\n",
        "\n",
        "for reg_val in reg_values: \n",
        "  print('Training with regularization value of {:f}'.format(reg_val))\n",
        "  args_dict = {'kernel_regularizer': regularizers.l2(reg_val)}\n",
        "  model, x_train, y_train, x_test, y_test = get_data_model(args_dict)\n",
        "  model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  history = model.fit(x_train, y_train, epochs=10, batch_size=256, verbose=0)\n",
        "  train_accuracy.append(history.history['acc'][-1])\n",
        "  test_accuracy.append(model.evaluate(x_test, y_test)[-1])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "plt.plot(reg_values, train_accuracy)\n",
        "plt.plot(reg_values, test_accuracy)\n",
        "plt.legend(['Training acc.', 'Test acc.'])\n",
        "plt.title('Accuracy vs. Regularization value')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Regularization value')\n",
        "plt.show()\n",
        "  "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "colab_type": "code",
        "id": "zv4c80XD_Yzr",
        "outputId": "7337306d-4cc6-48e5-d549-555a975a2d97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, regularizing the weights actually dropped the final accuracy, although we see how the gap between training and test accuracy decreases. "
      ],
      "metadata": {
        "colab_type": "text",
        "id": "liYOHIYT8YlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout\n",
        "\n",
        "Dropout is another regularization technique that can be applied. A Dropout layer deactivates during training some weights of the model by setting elements to 0 with a certain probability. The dropout value, i.e., the probability of disabling the input units, can be set as a parameter when defining a layer. In the evaluation phase, all of the weights are activated and dropout does not have any effect on the model. The dropout layer also scales during training the non-zero elements by 1/(1-prob_drop) to maintain a similar norm between training and evaluation. \n",
        "\n",
        "For example, in the following layer, the dropout value is 0.3, meaning 30% of the input data is switched off during training."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "FJ-J7tWRTQU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob_drop = 0.3\n",
        "drop = keras.layers.Dropout(prob_drop)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HEqZN0LyTv1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "x = np.random.random((1, 512))\n",
        "input_x = K.variable(x)\n",
        "# Set learning phase is used to manually setting\n",
        "# the phase (0 evaluation, 1 training)\n",
        "# Dropout only affects training, so we set it to 1\n",
        "K.set_learning_phase(1)\n",
        "y = K.eval(drop(input_x))\n",
        "print('Input (10 elements)')\n",
        "print(x[0,0:10])\n",
        "print('Output (10 elements)')\n",
        "print(y[0,0:10])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "colab_type": "code",
        "id": "Pqcb3TWCWe4e",
        "outputId": "a251ce90-dc18-488c-83ea-8c5003823edd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now check what percentage of elements have been set to 0, and what is the scaling value the other elements have."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "iXhWs8c1Xw3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Drop percentage, should be close to {:f}'.format(prob_drop))\n",
        "print(((y==0).sum())/(1.0*y.shape[1]))\n",
        "\n",
        "print('Scaling value, should be {:f}'.format(1/(1-prob_drop)))\n",
        "print(((y[y!=0]).sum())/(1.0*x[y!=0].sum()))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "colab_type": "code",
        "id": "8njS0-XRXVsc",
        "outputId": "331f6a96-398b-499a-fd20-2a04622a22bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Normalization\n",
        "\n",
        "Batch Normalization computes the mean and the standard deviation per channel, i.e. given a feature map of dimensionality $B\\times H\\times W\\times C$ ($B$ is batch size, $H,W$ spatial dimensions and $C$ number of feature channels) the layer computes the mean $\\mu$ and standard deviation $\\sigma$, where $\\mu$ and $\\sigma$ have dimensionality $1\\times 1\\times 1\\times C$. Then, the mean $\\mu$ and the standard deviation $\\sigma$ of the batch are used to standardize all of the dimensions of the input feature to follow a distribution with mean 0 and variance 1.\n",
        "The layer is defined in Keras by using:"
      ],
      "metadata": {
        "colab_type": "text",
        "id": "C98HL1rgVACR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_norm = keras.layers.BatchNormalization(axis=-1, input_shape=[10,10,1])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mt2E2uTqWGBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will generate a batch of 512x1 (a batch of 512 vectors of only 1 channel) using `np.random.random`, which is a uniform distribution under the $[0, 1)$ interval, resulting in mean 0.5 and variance 1/12. Finally, the batch normalization layer scales the distribution to have mean 0 and variance 1.\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "text",
        "id": "ax8OE0cbZA7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import BatchNormalization\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "K.set_learning_phase(1)\n",
        "x = np.random.random((512, 10, 10, 1))\n",
        "input_x = K.variable(x)\n",
        "y = K.eval(batch_norm(input_x))\n",
        "print('Input')\n",
        "print(x[:10, 0, 0, 0])\n",
        "print('Output')\n",
        "print(y[:10, 0, 0, 0])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "colab_type": "code",
        "id": "8IS_u-HKY1WR",
        "outputId": "f7d4c280-4a92-4247-ca02-9db0cddbf004"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input mean should be ~0.5 and var ~1/12=0.0833\n",
        "print(x.mean(), x.var())\n",
        "# Output mean should be ~0 and var ~1\n",
        "print(y.mean(), y.var())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "ILdlLzAtZ3sp",
        "outputId": "e6d1d2a3-16d5-454b-dceb-10fc24aaa100"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch normalization changes behaviour in evaluation mode. During training the layer also tracks the moving average of both mean and variance, which are then used to normalize the testing data without having to use the statistics from the testing batch."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "iDOne7XbrEhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HyperParameters Tuning\n",
        "\n",
        "When training our model, we need to decide the value of several hyperparameters, what regularization techniques we employ, or the loss used to train the model, among others. To decide these values we should not use as guidance the performance in the test set, as it may lead to overfitting to that set, and in turn to an erroneous estimate of the performance of the model in non-seen data. Hence, we use what is called a validation set, which we use to tweak the hyperparameters. To define a validation split automatically in Keras, we can use two relevant arguments in the `fit` method: `validation_split` and `validation_data`. The argument passed to `validation_split` (0 by default) determines the ratio of the training set for validation purposes. For example,    \n",
        "```\n",
        "model.fit(x_train, y_train, ..., validation_split=0.2)\n",
        "```\n",
        "uses 20% of `x_train` as validation data.\n",
        "\n",
        "Unfortunately, the validation data is randomly sampled and we can not fix the same splits during evaluations, so results are not directly comparable. To solve this problem, an option is using the `validation_data` argument, where we can pass directly the split of data we want to use as validation in the form of a tuple `(data, labels)`. \n",
        "\n",
        "Let's see how we can do the split. First, we load the data:"
      ],
      "metadata": {
        "colab_type": "text",
        "id": "NuF3Y5DoKylT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, x_train, y_train, x_test, y_test = get_data_model()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WWL_GXrnT_xS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we want to split `x_train` in training and validation, but we also need to follow the same partition for `y_train`. We can do so by using `numpy` functions:"
      ],
      "metadata": {
        "colab_type": "text",
        "id": "XKS6nnKAUBJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "\n",
        "# We initialize the pseudo-random number generator \n",
        "# in order to be able to reproduce the splits\n",
        "numpy.random.seed(1234)\n",
        "\n",
        "# We shuffle the indices in case the dataset follows an ordering\n",
        "# If we do not shuffle we may take only a subset of classes if the dataset is\n",
        "# ordered\n",
        "indices = numpy.random.permutation(x_train.shape[0])\n",
        "\n",
        "val_ratio = 0.2\n",
        "n_indices_train = int((1-val_ratio) * x_train.shape[0])\n",
        "train_idx, val_idx = indices[:n_indices_train], indices[n_indices_train:]\n",
        "x_train, x_val = x_train[train_idx,:], x_train[val_idx,:]\n",
        "y_train, y_val = y_train[train_idx], y_train[val_idx]\n",
        "print(x_train.shape[0], x_val.shape[0])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "dVMgy4n7UOdL",
        "outputId": "41dcacf1-3835-4a64-ace7-8772be03e289"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way is to use a package called `sklearn`, which contains a function called `train_test_split` that performs the split."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "yBf2hHC_WF-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's reload the data first\n",
        "model, x_train, y_train, x_test, y_test = get_data_model()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.20)\n",
        "print(x_train.shape[0], x_val.shape[0])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "Hy2DPVA9WFjk",
        "outputId": "4b67e776-98db-4338-9de8-c956ef04a5a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=2, batch_size=32, verbose=0, validation_data=(x_val, y_val))\n",
        "plot_history(history, 'categorical_accuracy')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "colab_type": "code",
        "id": "TulgsLVmWE6t",
        "outputId": "8651fbbb-92f5-42da-e4a1-844ed4436745"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compare the performance of several models by using the defined `x_val` and `y_val` variables. When we are happy with the performance in the validation split, we then evaluate the model in the unseen test data. Let's check if the accuracy in the test set is similar to the accuracy in the validation set in this case."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "rvWaSNdZV6e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy in the validation set is {:.2f}'.format(model.evaluate(x_val, y_val)[-1]))\n",
        "print('Accuracy in the test set is {:.2f}'.format(model.evaluate(x_test, y_test)[-1]))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "colab_type": "code",
        "id": "UMRAQsCYVrcg",
        "outputId": "5c53d5e3-f6d5-4fec-9e79-38f6d4a444b7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know how to define a validation set and how to use it during the training process. An important step now is how to retrieve the model with the highest validation performance during training. We already mentioned this in the introductory Keras tutorial, but let's explain this again now that we know how to define a validation split. By default, Keras returns the last model after training $N$ epochs. We can instead choose to retrieve the model with the best validation performance by using the [`EarlyStopping`](https://keras.io/callbacks/#earlystopping) callback."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "9DFkIWei6hse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0, patience=4, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=20, batch_size=32, verbose=1, validation_data=(x_val, y_val), callbacks=[early_stop])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "colab_type": "code",
        "id": "QFfNridt72qT",
        "outputId": "af5611f6-4bc8-4980-b726-8dbde400040e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We check that the performance of the saved model is the same as the obtained in the best epoch in terms of validation accuracy."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "VtujLcUPAc7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_val, y_val)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "LPiaL47KAY-e",
        "outputId": "0404e634-27ab-472d-8d31-e8d855464105"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the most important parameters to tweak is the training rate, which controls the update step performed during the backpropagation. Keras provides two callbacks that allow us to modify the learning rate during training. One is `LearningRateScheduler`, which allows us to define a rule to vary the learning rate depending on the epoch. For example, using the `lr_scheduler` function (found [here](https://stackoverflow.com/questions/39779710/setting-up-a-learningratescheduler-in-keras)), we can modify the loss function so that every 3 epochs is multiplied by 0.1."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "NMd2DVJCGmE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_scheduler(epoch, lr):\n",
        "    decay_rate = 0.1\n",
        "    decay_step = 3\n",
        "    if epoch % decay_step == 0 and epoch:\n",
        "        return lr * decay_rate\n",
        "    return lr\n",
        "    \n",
        "lrate = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "model, x_train, y_train, x_test, y_test = get_data_model()\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "initial_lr = K.get_value(model.optimizer.lr)\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[lrate])\n",
        "print('Initial Learning Rate: {:.4f}'.format(initial_lr))\n",
        "print('Final Learning Rate: {:.10f}'.format(K.eval(model.optimizer.lr)))\n",
        "plot_history(history, 'categorical_accuracy')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "colab_type": "code",
        "id": "AC9in_Zhfnly",
        "outputId": "0974337d-8ed8-459d-9597-9475ffcf66b6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the progress of the learning rate in each epoch to check how the learning rate is decreased every three epochs as we defined in `lr_scheduler`."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "jAF6VK2gkdRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = history.history['lr']\n",
        "plt.plot(range(1, len(learning_rate)+1), learning_rate)\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "colab_type": "code",
        "id": "jtugFeDVj8HD",
        "outputId": "7f1ac2d3-ead7-40ab-a801-7503a05cba49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another callback provided is `ReduceLROnPlateau`, which reduces the learning rate whenever a given metric has stopped improving. There are 5 important arguments:\n",
        "\n",
        " * `monitor`: we specify the metric we want to track\n",
        " * `patience`: number of epochs without improvement before reducing lr\n",
        " * `factor`: the new learning rate will be `new_lr = lr * factor`\n",
        " * `min_lr`: sets the minimum lr\n",
        " * `min_delta`: margin to define when the metric has stopped improving\n",
        " "
      ],
      "metadata": {
        "colab_type": "text",
        "id": "XAPrOzm0iDTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=1, min_lr=0.00001, min_delta = 0.01)\n",
        "model, x_train, y_train, x_test, y_test = get_data_model() \n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "initial_lr = K.get_value(model.optimizer.lr)\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[reduce_lr])\n",
        "print('Initial Learning Rate: {:.4f}'.format(initial_lr))\n",
        "print('Final Learning Rate: {:.10f}'.format(K.eval(model.optimizer.lr)))\n",
        "plot_history(history, 'categorical_accuracy')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "colab_type": "code",
        "id": "os7b7Oh0TO7m",
        "outputId": "154d49c5-6432-4b32-846b-09246e09df36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we check how the learning rate has changed. You can check that the learning has indeed decreased when the `val_loss` has not improved by more than 0.01 until it reached the `min_lr` value."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "FX9W-7yNoyvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = history.history['lr']\n",
        "plt.plot(range(1, len(learning_rate)+1), learning_rate)\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "colab_type": "code",
        "id": "BVNFkE-qnKVk",
        "outputId": "6f8e8d02-d286-41cd-cca7-b55f97d23f45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Searching for the right set of parameters\n",
        "\n",
        "Apart from the learning rate, there are several hyperparameters we can tune: the optimizer parameters (momentum, beta, rho, decay), the dropout rate, the number of neurons/feature maps, batch size, regularization weights, etc...  After some time working with the models, you gain an intuition of what set of parameters work better. However, performing a proper search of hyperparameters may improve the results. A way to do this (among several others) is performing a search of parameters. Several available packages can help you with hyperparameter optimization in Keras, the one we will use is called [`talos`](https://github.com/autonomio/talos)."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "ugiRVHj-j8UX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install talos==0.4.9"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "Az5-RBT1zFX8",
        "outputId": "2f6d0f7b-38f0-4985-fbea-2564568646b9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we show a quick example of how to do it. We set only 2 epoch of training and `grid_downsampling=0.05`, which controls the number of sets of hyperparameters tested."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "a2w5UNq0_I0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import talos as ta\n",
        "_, x_train, y_train, x_test, y_test = get_data_model()\n",
        "\n",
        "def model_scan(x_train, y_train, x_val, y_val, params):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(params['first_neuron'], input_shape=(784,), activation=params['activation']))\n",
        "  model.add(Dropout(params['dropout']))\n",
        "  model.add(Dense(10, activation=params['last_activation']))\n",
        "\n",
        "  model.compile(optimizer=params['optimizer'](lr = params['lr']),\n",
        "                loss=params['losses'],\n",
        "                metrics=['categorical_accuracy'])\n",
        "  \n",
        "  out = model.fit(x_train, y_train,\n",
        "                  batch_size=params['batch_size'],\n",
        "                  epochs=params['epochs'],\n",
        "                  verbose=0,\n",
        "                  validation_data=[x_val, y_val])\n",
        "  \n",
        "  return out, model\n",
        "p = {}\n",
        "p['lr'] = [0.0001, 0.001]\n",
        "p['first_neuron'] = [8, 16, 32, 64, 128]\n",
        "p['batch_size'] =  [20, 40]\n",
        "p['epochs'] =  [2]\n",
        "p['dropout'] =  [0, 0.40, 0.7]\n",
        "p['weight_regularizer'] = [None]\n",
        "p['optimizer'] =  [keras.optimizers.Adam, keras.optimizers.SGD]\n",
        "p['losses'] =  ['categorical_crossentropy']\n",
        "p['activation'] = ['relu', 'tanh']\n",
        "p['last_activation'] =  ['softmax']\n",
        "## grid_downsample limits the number of combinations run in the search\n",
        "## to the fraction set (e.g. 0.05 runs 5% of the possible combinations) \n",
        "h = ta.Scan(x_train, y_train,\n",
        "          params=p,\n",
        "          model=model_scan,\n",
        "          grid_downsample=0.05,\n",
        "          print_params=True,\n",
        "          disable_progress_bar=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "colab_type": "code",
        "id": "Pm2XxaG_zPT-",
        "outputId": "167f14ae-bb1f-478a-85f3-c3cd68056097"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we check the results of the experiment."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "5FCxXrdxCXRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from talos import Reporting\n",
        "r = Reporting(h)\n",
        "# returns the results dataframe\n",
        "r.data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "colab_type": "code",
        "id": "xARWdCT0_-Ig",
        "outputId": "ebf973ee-7977-4e36-a225-516221403160"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Talos' documentation](https://autonomio.github.io/docs_talos/#introduction) provides more information about the package."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "cBBz_MMS_ezr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data augmentation\n",
        "Deep learning models are data-hungry, tend to overfit with small training sets and its performance benefit from large training sets. A way to synthetically create more data is using data augmentation. Now, we will show some examples of data augmentation for images.\n",
        "### Images\n",
        "Data augmentation techniques such as rotation, color jittering, scale or cropping are usually applied in deep learning pipelines for vision applications. The idea is to take as input an image, apply a transformation to it, and then use it for training.\n",
        "\n",
        "Keras includes a preprocessing module, with all [these transformations](https://keras.io/preprocessing/image/) implemented. The preprocessing module can be imported by doing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "text",
        "id": "xm0oVnMQWLZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "n4glvOHvh8qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we need to fit it to the input data using `fit` (which computes statistics from the dataset, such as means and std values) and use `flow` to apply the transformations to the input data. In the function `plot_data_augmentation` defined in the code cell below we can see the process of fitting the data and then using flow to iterate over the generator.\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "text",
        "id": "2vftBzsolQcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_data_augmentation(augmentation_gen = ImageDataGenerator()):\n",
        "  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  augmentation_gen.fit(np.expand_dims(x_train, -1))\n",
        "  for X_batch, y_batch in augmentation_gen.flow(np.expand_dims(x_train, -1), y_train, batch_size=5, shuffle=False):\n",
        "    for i in range(0, 5):\n",
        "      plt.subplot(150 + 1 + i)\n",
        "      plt.imshow(X_batch[i, :].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
        "    # show the plot\n",
        "    plt.show()\n",
        "    break"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yTLpcR43IHpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now visualize some of the transformations available to use.\n",
        "\n",
        "First, we plot some images without any transformations applied for comparison."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "OoJYjjapiISI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data_augmentation()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "colab_type": "code",
        "id": "MyoZHyOTheyU",
        "outputId": "6cb02f6a-69fa-4a57-a988-604a767d1cac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rotation\n",
        "A standard transformation is to rotate the image. We can do so by initializing ImageDataGenerator with `rotation_range=rot_val`."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "eFXEC5EejxWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We first define the transformation we want to apply\n",
        "augmentation_gen = ImageDataGenerator(rotation_range=90)\n",
        "plot_data_augmentation(augmentation_gen)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "colab_type": "code",
        "id": "hjgfZ1z3j46X",
        "outputId": "123290d1-7d2f-42eb-f415-9a4af3644d78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shift\n",
        "\n",
        "We can define a maximum range of both horizontal (`width_shift_range`) and vertical (`height_shift_range`) shift."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "Schj43v4ioTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmentation_gen = ImageDataGenerator(width_shift_range=0.3, height_shift_range=0.3)\n",
        "plot_data_augmentation(augmentation_gen)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "colab_type": "code",
        "id": "dbzq69FfjLd9",
        "outputId": "1da18a2a-a13e-445d-b3cb-9b4c71cbb7f2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zooming\n",
        "Zooming into the image can be done with `zoom_range`."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "7uhYKLIPI1LA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmentation_gen = ImageDataGenerator(zoom_range=0.4)\n",
        "plot_data_augmentation(augmentation_gen)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "colab_type": "code",
        "id": "NGvZzyL0HQjF",
        "outputId": "67391396-96f1-4ccf-ff13-3c7f32b4c17b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flip\n",
        "\n",
        "We can define either horizontal flip (`horizontal_flip`) or vertical (`vertical_flip`).  "
      ],
      "metadata": {
        "colab_type": "text",
        "id": "845GvrICI9IX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmentation_gen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
        "plot_data_augmentation(augmentation_gen)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "colab_type": "code",
        "id": "m6emZPbsHXOV",
        "outputId": "80bd1ffc-eb74-468a-c67f-bf53c52a6dd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining transformations\n",
        "We can combine all the transformations and train a model. The `ImageDataGenerator` is a generator, so we need to use the method `fit_generator`, which is explained [in the documentation](https://keras.io/models/sequential/)."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "GOE33AOLJdPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmentation_gen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "augmentation_gen.fit(np.expand_dims(x_train, -1))\n",
        "train_gen = augmentation_gen.flow(np.expand_dims(x_train, -1), keras.utils.to_categorical(y_train))\n",
        "model = Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=(28, 28, 1)))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "model.fit_generator(train_gen, samples_per_epoch=len(x_train), epochs=10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "colab_type": "code",
        "id": "Xqb15Ga0EsC9",
        "outputId": "c5d20f3f-3e67-48e5-8536-bd6c5404a9a5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In these examples we showed practical examples of data augmentation for images. However, other modalities, such as text or audio can also benefit from data augmentation as shown in the lecture slides."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "65Ynz4OOee5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorboard\n",
        "Tensorboard is quite useful to monitor the different metrics in real time. Both Tensorflow and Pytorch users (using the TensorboardX module) can use it. Tensorboard can be used in Keras by using the Tensorboard callback available ([documentation here](https://keras.io/callbacks/)).\n",
        "\n",
        "However, to make it working in a Colab environment, we need to follow a different process, which is explained [here](https://medium.com/@tommytao_54597/use-tensorboard-in-google-colab-16b4bb9812a6). When you run the following piece of code, a Tensorboard link will be displayed. If you click on it, and you will be redirected to the Tensorboard site."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "9Q8t9ZSK2nmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardcolab\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "colab_type": "code",
        "id": "IMK1K_sd24ee",
        "outputId": "efd36cae-4bb3-4e3a-f784-6e736b29faa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorboardcolab as tbc\n",
        "K.clear_session()\n",
        "tboard = tbc.TensorBoardColab()\n",
        "from tensorboardcolab import TensorBoardColabCallback"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "uwlns-E_6PSw",
        "outputId": "6038cb21-2e58-4c0e-8a13-ad79152356d1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can use a callback function to show the training progress in the given link. "
      ],
      "metadata": {
        "colab_type": "text",
        "id": "PYSa2XXG4Cb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, x_train, y_train, x_test, y_test = get_data_model()\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32,  validation_split=0.2, callbacks=[TensorBoardColabCallback(tboard)])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "colab_type": "code",
        "id": "zFh9bJyJ4f7W",
        "outputId": "d071fdb6-db9b-43df-9005-4e7d03641015"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Tensorboard website you should see two sections: *Scalars* and *Graph*. In *Scalars* there is the plot with the training and validation loss per epoch, and in *Graph* you should have the graph of your model. You can also plot images, histograms, distributions and other things in Tensorboard, which makes it quite useful to keep track of the training progress. You will not have to use Tensorboard for the tutorials, however it is a nice visualization tool that can be useful for future projects."
      ],
      "metadata": {
        "colab_type": "text",
        "id": "N0x9hnGhUW9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "You will use the Wine Quality (http://archive.ics.uci.edu/ml/datasets/Wine+Quality) dataset. Use only the red wine data. The goal is to find the quality score of some wine based on its attributes. Write your code in a script neural_net.py. \n",
        "\n",
        "1)\tFirst, download the winequality-red.csv file, load it, and divide the data into a training and test set using approximately 50% for training. Standardize the data, by computing the mean and standard deviation for each feature dimension using the training set only, then subtracting the mean and dividing by the stdev for each feature and each sample. Append a 1 for each feature vector, which will correspond to the bias that our model learns. Set the number of hidden units, the number of iterations to run, and the learning rate. \n",
        "2)\tCall the backward function to construct and train the network. Use 1000 iterations and 30 hidden neurons. \n",
        "3)\tThen call the forward function to make predictions and compute the root mean squared error between predicted and ground-truth labels. Report this number in a file lab3.pdf/docx \n",
        "4)\tExperiment with three different values of the learning rate. For each, plot the error over time (output by backward above). Include these plots in your report.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "2020_04_Network_Training.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}