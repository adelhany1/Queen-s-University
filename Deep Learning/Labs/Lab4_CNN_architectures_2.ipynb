{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatchLab-Imperial/deep-learning-course/blob/master/2020_05_CNN_architectures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IhXUenVgBmjf"
      },
      "source": [
        "# Introduction to Convolutional Neural Networks Architectures\n",
        "In this lab, we will learn about well-known CNN architectures in the field of computer vision and how to implement them in Keras. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "auwuthntar8h"
      },
      "source": [
        "# CNNs in Image Classification\n",
        "We will start this part by presenting the networks that have been widely used for image classification.  (One of the most famous problems among the computer vision community is the annual software contest run by the ImageNet project, **ImageNet Large Scale Visual Recognition Challenge (ILSVRC)**. This task evaluates how well an algorithm does in the tasks of object category classification and detection on hundreds of object categories and millions of images. \n",
        "\n",
        "As ImageNet is a massive dataset, we will use instead CIFAR10 dataset, which is directly available on Keras. \n",
        "\n",
        "CIFAR10 dataset contains 10 different classes and 32x32 images, instead of 224x224 as in ImageNet. We will show how to resize the data to be able to use some of the following architectures, as not all of the CNN accepts arbitrary input sizes. The ones that do, will use the original CIFAR10 input size. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "i87in95Fj0zF"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gaQUxiIirq3r"
      },
      "source": [
        "To resize the dataset, and as a CIFAR10 224x224 version would not fit in our RAM, we need to create a preprocessing step before the first layer. This step will resize the input batch to the required size. To do so, we use the `Lambda` layer in Keras, which wraps any expression (in this case `resize_images`) in a layer. We can see how this function looks like for a single batch of 32 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "colab_type": "code",
        "id": "1FihA5egr6qs",
        "outputId": "f3acfe73-e81c-4bfa-89be-1eac9ffd32cc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.ndimage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.layers import Lambda, Input\n",
        "from keras.models import Model, Sequential\n",
        "import tensorflow as ktf\n",
        "#ktf.logging.set_verbosity(ktf.logging.ERROR)\n",
        "\n",
        "\n",
        "inp = Input(shape=(32, 32, 3))\n",
        "out = Lambda(lambda image: ktf.image.resize(image, (224, 224)))(inp)\n",
        "\n",
        "model = Model(inputs=inp, outputs=out)\n",
        "\n",
        "x_train_resized = model.predict(x_train[:32, ...])\n",
        "print(x_train_resized.shape)\n",
        "\n",
        "# Visualization purposes\n",
        "x_train_resized = np.asarray(x_train_resized, dtype = np.int)\n",
        "    \n",
        "# Let's visualize some examples\n",
        "N=2\n",
        "start_val = 0 # pick an element for the code to plot the following N**2 values\n",
        "fig, axes = plt.subplots(N,N)\n",
        "items = list(range(0, 100))\n",
        "for row in range(N):\n",
        "  for col in range(N):\n",
        "    idx = start_val+row+N*col\n",
        "    axes[row,col].imshow(x_train_resized[idx], cmap='gray')\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "    y_target = int(y_train[idx])\n",
        "    target = str(items[y_target])\n",
        "    axes[row,col].set_title(target)\n",
        "    axes[row,col].set_xticks([])\n",
        "    axes[row,col].set_yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qw0FyvNCizwx"
      },
      "source": [
        "Let's get ready the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "colab_type": "code",
        "id": "eGI2w-ssjAmZ",
        "outputId": "f639ebf7-d15d-4579-ac0a-6e3b7ce95baf"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "y_train=to_categorical(y_train,num_classes)\n",
        "y_test=to_categorical(y_test,num_classes) \n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "\n",
        "print('Original training data shape: {}'.format(x_train.shape))\n",
        "print('Training label shape: {}'.format(y_train.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zwo-H2oSfQ0s"
      },
      "source": [
        "## AlexNet\n",
        "\n",
        "AlexNet made a huge impact in 2012 when it reduced the top-5 error (i.e. the correct class is not among the top-5 predictions) from 26% to 15.3% in the ImageNet challenge. The second place was close to 26.2%, and it was not a CNN based system. AlexNet shares a lot with its predecessor architecture, [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) by Yann LeCun et al. However, in this case, the computational complexity was more plausible to deal with that it was years before, therefore, authors decided to make the architecture bigger: more convolutional layers and more filters. \n",
        "\n",
        "The architecture designed in their paper:\n",
        "\n",
        "![texto alternativo](https://i.ibb.co/QFG561f/0-x-POQ3bt-Z9r-QO23-LK.png)\n",
        "\n",
        "The network consisted of convolutional layers with kernels of 11x11, 5x5 and 3x3. The architecture uses layers with strides, max poolings, dropouts, ReLU activation functions, and three dense layers at the end. The original network was split in two, as seen in the figure above, however, due to GPU constraints authors needed to train it on two separate GPUs. We found a great implementation in [Rizwan's blog](https://engmrk.com/alexnet-implementation-using-keras/), where it presented an AlexNet model without the split concept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7U4lWS9WyBP0"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Resize input image\n",
        "model.add(Lambda(lambda x: ktf.image.resize(x, (224, 224)), input_shape=(32,32,3)))\n",
        "\n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "\n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "\n",
        "# 3rd Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# 4th Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# 5th Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "\n",
        "# Passing it to a Fully Connected layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# 1st Fully Connected Layer\n",
        "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# 2nd Fully Connected Layer\n",
        "model.add(Dense(4096))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# 3rd Fully Connected Layer\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xSfj3T_T0ZZ9"
      },
      "source": [
        "Now, we are ready to train this AlexNet model on CIFAR10. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "colab_type": "code",
        "id": "BsOYPKwB0k52",
        "outputId": "da8ddbe7-caab-4f05-e6f1-9165581fc11b"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, validation_split=0.2, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u-LaLUyEGvfM"
      },
      "source": [
        "Let's test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "colab_type": "code",
        "id": "q0XC8B0fGxSu",
        "outputId": "4c8f6200-8a86-4e97-c248-73bcd6ae8738"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(x_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7zyJoeM76R6W"
      },
      "source": [
        "Training this kind of architectures in big datasets is time-consuming. We might need to train for several hours to start getting good results. That is why Keras offers several pre-trained models on the ImageNet dataset. Those models can be used directly for image prediction, image classification, feature extraction, or fine-tuning, among others, without the need of spending long hours of training. Even if the task or the dataset is different, using the pre-trained weights as initialization for the training process provides usually quite better results (as well as less time needed for training and better accuracy) than using a random initialization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wks8tkdjdtgV"
      },
      "source": [
        "# Loading Pre-trained Models in Keras\n",
        "\n",
        "As mentioned, Keras contains many models that have been pre-trained in the ImageNet dataset to solve the ILSVRC competition. Those deep learning models are made available alongside their pre-trained weights, which has become a common practice to initialize networks. You can learn more about it [here](https://keras.io/applications/). \n",
        "\n",
        "We will show how to instantiate the models and load their weights. Afterward, we will modify the last layer to classify only the 10 classes belonging to CIFAR10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GWbK1gE99o7K"
      },
      "source": [
        "## VGGNet\n",
        "\n",
        "VGGNet is an architecture presented by Simonyan and Zisserman in 2014. VGGNet is similar to previous AlexNet, however, it only contains 3x3 convolutional kernels and many more filters. It is widely used for extracting features, not only for image classification but for many other computer vision domains, such as feature representation, style transfer or the image description. You can check the [paper](https://arxiv.org/pdf/1409.1556.pdf) for further details. Next image shows VGG16 architecture ([source](https://www.cs.toronto.edu/~frossard/post/vgg16/)):\n",
        "\n",
        "![](https://www.cs.toronto.edu/~frossard/post/vgg16/vgg16.png)\n",
        "\n",
        "The training time of the architecture is massive since it has more than 130 million parameters. Thankfully, we can find it already pre-trained in Keras. Keras has the two proposed versions, VGG16 and VGG19, where the difference lies in the number of weight layers within the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CPpzPdYAREBb"
      },
      "outputs": [],
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "model = VGG16(input_shape=(224, 224, 3))\n",
        "\n",
        "# Optionally, uncomment the following line to display the full model\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t3ZLsBmIRpqQ"
      },
      "source": [
        "If we want to load weights from ImageNet we only need to initialize the model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "B29erHYWRp9Y"
      },
      "outputs": [],
      "source": [
        "model = VGG16(weights='imagenet', input_shape=(224, 224, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z6TMSj9r3Dkr"
      },
      "source": [
        "Now, we want to change the last layer for one dense layer with 10 activation neurons. This new layer will allow us to perform classification on CIFAR10. Besides, we will freeze all pre-trained layers and only allow the new dense layer to be trained. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "P1c26De_3ihg"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "\n",
        "#Load the VGG\n",
        "model = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze all the layers\n",
        "for layer in model.layers[:]:\n",
        "    layer.trainable = False\n",
        "    \n",
        "output = model.get_layer('fc2').output\n",
        "output = Dense(units=10, activation='softmax')(output)\n",
        "model = Model(model.input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I3gLnH3p4Wl6"
      },
      "source": [
        "Let's see the model now, and confirm that the last layer is not there anymore, and instead, we have a new dense layer with only 10 activations. Also, as we have frozen the weights of the model, we should see only the trainable parameters belonging to the new dense layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "colab_type": "code",
        "id": "MX2lK4YB4leZ",
        "outputId": "148bfa80-e7a5-438a-f58c-7d57d4ac8d85"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1p1qVDFC-KoP"
      },
      "source": [
        " As we have done before, we can add the lambda function to resize the images within our Keras model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "twaI6se2-I3g"
      },
      "outputs": [],
      "source": [
        "newInput = Input(batch_shape=(None, 32, 32, 3))\n",
        "resizedImg = Lambda(lambda image: ktf.image.resize(image, (224, 224)))(newInput)\n",
        "newOutputs = model(resizedImg)\n",
        "model = Model(newInput, newOutputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JdIufD1rDyig"
      },
      "source": [
        "If we want to use images of 32x32 we need to set the argument `include_top` to `False`, which will remove all dense layers at the end of the architecture. The dense layers in VGG are the ones that limit the input size to be 224x224 as they need a fixed dimension as input. Also, note that due to the MaxPooling operations, we need inputs of at least 32x32. If we remove the dense layers, we need to include new ones before starting the training process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vDjgj0Q4Ds1-"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense, Flatten\n",
        "\n",
        "model = VGG16(include_top=False, input_shape=(32,32,3), weights='imagenet')\n",
        "\n",
        "# Freeze all the layers\n",
        "for layer in model.layers[:]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add Dense layer as in VGG16\n",
        "output = model.output\n",
        "output = Flatten()(output)\n",
        "output = Dense(units=4096, activation='relu')(output)\n",
        "output = Dense(units=4096, activation='relu')(output)\n",
        "output = Dense(units=10, activation='softmax')(output)\n",
        "model = Model(model.input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Izc03Dfc7lL_"
      },
      "source": [
        "Now we have trainable parameters belonging to the last layers, so we are ready to fine-tune the VGG architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "colab_type": "code",
        "id": "8IRy6UZf70rU",
        "outputId": "8b97cd79-e5b2-4e6e-85b3-9ea904cbc168"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "model.fit(x_train, y_train, epochs=3, batch_size=32,  validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xybWrAjbBcyh"
      },
      "source": [
        "Let's evaluate it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "colab_type": "code",
        "id": "nW75Tn5b8LcL",
        "outputId": "2c7d3b93-5927-4aca-8d8f-81c983f647b5"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(x_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rSaBmsu7IKD9"
      },
      "source": [
        "## GoogLeNet / Inception v1\n",
        "\n",
        "Google presented [GoogLeNet](https://arxiv.org/pdf/1409.4842.pdf) in 2014, the same year that VGGNet was introduced.  GoogLeNet won the ImageNet competition achieving a top-5 error rate of 6.67%, almost betting the human error rate (5%). \n",
        "\n",
        "GoogLeNet introduced many ideas that helped the development of current state-of-the-art architectures. Instead of stacking more and more CNN layers, GoogLeNet introduced what the called inception modules:\n",
        " \n",
        "![](https://i.ibb.co/JKqptrj/Googlenet-inception.png)\n",
        "\n",
        "Those modules apply convolutions with 3 different sizes of kernels (1x1, 3x3 and 5x5) at the same level. The idea behind those modules is based on the premise that significant information can be presented in images at numerous different sizes. Therefore, by using a multi-scale approach, they are more likely to capture meaningful information. Moreover, they claimed that by designing a wider, instead of deeper, architecture helps the gradients to go through the entire network easily.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1600/1*uW81y16b-ptBDV8SIT1beQ.png)\n",
        "\n",
        "The architecture consisted of 9 inception modules stacked linearly, containing 22 deep CNN layers, however, the total number of parameters was reduced to 4 millions! Authors also included two auxiliary classifiers in the middle part of the architecture to avoid the vanishing gradient problem.\n",
        "\n",
        "You can find a Keras implementation [here](https://gist.github.com/joelouismarino/a2ede9ab3928f999575423b9887abd14)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aGCXlBgVIxP6"
      },
      "source": [
        "Original Inception v1 was a complex and heavily engineered architecture, and therefore many tricks were presented to push its performance, both in terms of speed and accuracy. The constant evolution of the architecture leads to the creation of several versions, a list and explanation of their differences can be found in [this blog](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202).  \n",
        "\n",
        "In Keras, the Inception v3 architecture is available, together with its pre-trained weights on ImageNet. It can be initialized as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "colab_type": "code",
        "id": "Lw6quKrOpGjT",
        "outputId": "de9c9518-ef2e-43da-8509-3b3a78b24bd6"
      },
      "outputs": [],
      "source": [
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "model = InceptionV3(include_top=True, weights='imagenet', classes=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rxmx-iMnp4Wz"
      },
      "source": [
        "We can see its performance in CIFAR10. First of all, we will resize images to 75x75  which is the minimum size required by Inception v3 to work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "colab_type": "code",
        "id": "jkGHeRxZrwWL",
        "outputId": "1d5ff168-e77f-417b-f37b-750fdc2602ac"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Lambda, Input\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "model = InceptionV3(include_top=False, input_shape=(75, 75, 3), weights='imagenet')\n",
        "\n",
        "# Resize Input images to 75x75\n",
        "newInput = Input(batch_shape=(None, 32, 32, 3))\n",
        "resizedImg = Lambda(lambda image: ktf.image.resize(image, (75, 75)))(newInput)\n",
        "newOutputs = model(resizedImg)\n",
        "model = Model(newInput, newOutputs)\n",
        "\n",
        "# Freeze all the layers\n",
        "for layer in model.layers[:]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add Dense layer to classify on CIFAR10\n",
        "output = model.output\n",
        "output = GlobalAveragePooling2D()(output)\n",
        "output = Dense(units=10, activation='softmax')(output)\n",
        "model = Model(model.input, output)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fZwcxCiGxRQ-"
      },
      "source": [
        "We can train Inception v3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "colab_type": "code",
        "id": "n_kaiLcGxE8V",
        "outputId": "08ae0c16-c922-4fd7-ac76-a87121aca019"
      },
      "outputs": [],
      "source": [
        "print(x_train.shape)\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "model.fit(x_train, y_train, epochs=3, batch_size=32,  validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9PbyNXbWxT_U"
      },
      "source": [
        "And check its accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "colab_type": "code",
        "id": "dRB4GcuJxbp_",
        "outputId": "c62ac533-b728-42b0-80f4-d40e4249c851"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(x_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eCV0NEx-F-0D"
      },
      "source": [
        "## ResNet\n",
        "\n",
        "In 2015 the winner of the ImageNet challenge was the Residual Neural Network ([ResNet](https://arxiv.org/pdf/1512.03385.pdf)) architecture. Similar to Inception, ResNet is built by micro-architectures modules, called residual blocks. Those blocks introduced skip connections, which allowed to train huge architectures (152 layers) while still having lower complexity than VGGNet. Those residual connections also allowed them to create deeper architectures since the gradient could backpropagate easier through the skip connections. Next image is from [Das' blog](https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5) and shows ResNet architecture with its skip connections:\n",
        "\n",
        "![texto alternativo](https://cdn-images-1.medium.com/max/800/0*pkrso8DZa0m6IAcJ.png)\n",
        "\n",
        "Keras offers the implementation of ResNet50 pre-trained on ImageNet. In addition to ResNet50, deeper versions of it, ResNet101 or ResNet152, are widely used nowadays. The main difference lies in the number of layers, e.g., ResNet50 is a 50 layer Residual Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "L3AJ5oF0dz64"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "\n",
        "model = ResNet50(include_top=False, input_shape=(32,32,3), weights='imagenet')\n",
        "\n",
        "# Freeze all the layers\n",
        "for layer in model.layers[:]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add Dense layer to classify on CIFAR10\n",
        "output = model.output\n",
        "output = GlobalAveragePooling2D()(output)\n",
        "output = Dense(units=10, activation='softmax')(output)\n",
        "model = Model(model.input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yC5S2IONeaWd"
      },
      "source": [
        "Let's check the performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "colab_type": "code",
        "id": "XRJ80Qi2ef0U",
        "outputId": "5d607958-9035-48a9-af26-f38a188489ce"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "model.fit(x_train, y_train, epochs=3, batch_size=32,  validation_split=0.2)\n",
        "\n",
        "score = model.evaluate(x_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ILGzCzQlD26"
      },
      "source": [
        "## Xception, DenseNet, InceptionResNet ...\n",
        "\n",
        "Keras offers many more network implementations trained on ImageNet. You can check the [documentation](https://keras.io/applications/) to see all of them. Besides, the documentation reports their accuracies, number of parameters and depth of each architecture. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s_mCvUoEZYmd"
      },
      "source": [
        "# CNN on Others Domains\n",
        "\n",
        "Besides the success of CNNs in classification problems, CNNs have been used in many other domains, such as object detection, facial recognition, image segmentation, image captioning, image generation, among others. We present now two state-of-the-art architectures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q5AKwf_Hl-Gv"
      },
      "source": [
        "## Denoising with UNet\n",
        "\n",
        "[UNet](https://arxiv.org/pdf/1505.04597.pdf) was presented in 2015 for biomedical image segmentation, but since then, it has been employed in many different tasks such as as semantic segmentation, generative models, denoising... The idea is similar to an encoder-decoder layout. The first part of the network encodes the image and reduces it to a map of smaller size, forcing it to capture the important information. Then, the next part is in charge of decoding the image from that vector.\n",
        "\n",
        "UNet presents also skip connections between the encoder and the decoder to allow easier flow of information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "keL5CmHpAGeb"
      },
      "source": [
        "Now let's define the UNet architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ZoYzGXVRnP5k"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, UpSampling2D, concatenate \n",
        "from keras.models import Model\n",
        "\n",
        "inputs = Input((32,32,3))\n",
        "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "drop4 = Dropout(0.5)(conv4)\n",
        "pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "## Now the decoder starts\n",
        "\n",
        "up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "merge6 = concatenate([drop4,up6], axis = 3)\n",
        "conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "\n",
        "up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "merge7 = concatenate([conv3,up7], axis = 3)\n",
        "conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "\n",
        "up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "merge8 = concatenate([conv2,up8], axis = 3)\n",
        "conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "\n",
        "up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "merge9 = concatenate([conv1,up9], axis = 3)\n",
        "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "conv10 = Conv2D(3, 3,  padding = 'same')(conv9)\n",
        "\n",
        "model = Model(inputs = inputs, outputs = conv10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BfMCDfIJAdYY"
      },
      "source": [
        "We have CIFAR10 images loaded from previous experiments. We are going to add some Gaussian noise (you can play with the scale of the noise) and then train the UNet to remove the noise from the images. To do so, we train with the noisy image as input and the clean image as the target. We use the Mean Absolute Error as the loss function to optimize the architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "colab_type": "code",
        "id": "OarMCDhS7UjV",
        "outputId": "536f64fa-f3cb-4eaf-e80a-02458c39cd17"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "x_train_noise = x_train + np.random.normal(size = x_train.shape, scale = 0.1)\n",
        "x_test_noise = x_test + np.random.normal(size = x_test.shape, scale = 0.1)\n",
        "\n",
        "model.compile(optimizer = 'Adam', loss = 'mean_absolute_error', metrics = ['mae'])\n",
        "model.fit(x_train_noise, x_train, batch_size=32, epochs=3, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3df46sxxpf4S"
      },
      "outputs": [],
      "source": [
        "pred = model.predict(x_test_noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CULZTfRdBk3C"
      },
      "source": [
        "We now visualize the noisy image next to the output of the UNet. The output images are a little bit blurred, but the noise has been reduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "colab_type": "code",
        "id": "O0H2dGQ0p98I",
        "outputId": "68344307-4867-4e61-b6a3-e63ce49bc9f9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N=5\n",
        "start_val = 0# pick an element for the code to plot the following N**2 values\n",
        "fig, axes = plt.subplots(N,N)\n",
        "for row in range(N):\n",
        "  for col in range(N):\n",
        "    idx = start_val+row+N*col\n",
        "    \n",
        "    im = np.concatenate((np.clip(x_test_noise[idx], 0, 1), np.clip(pred[idx], 0, 1)), 1)\n",
        "    axes[row,col].imshow(im)\n",
        "    axes[row,col].set_xticks([])\n",
        "    axes[row,col].set_yticks([])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "2020_05_CNN_architectures.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
