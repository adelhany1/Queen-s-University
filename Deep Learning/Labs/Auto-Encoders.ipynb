{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto-Encoders\n",
    "\n",
    "A typical use of a Neural Network is a case of supervised learning. It involves training data which contains an output label. The neural network tries to learn the mapping from the given input to the given output label. But what if the output label is replaced by the input vector itself? Then the network will try to find the mapping from the input to itself. This would be the identity function which is a trivial mapping.\n",
    "\n",
    "But if the network is not allowed to simply copy the input, then the network will be forced to capture only the salient features. This constraint opens up a different field of applications for Neural Networks which was unknown. The primary applications are dimensionality reduction and specific data compression.\n",
    "\n",
    "The network is first trained on the given input. The network tries to reconstruct the given input from the features it picked up and gives an approximation to the input as the output. The training step involves the computation of the error and backpropagating the error. The typical architecture of an Auto-encoder resembles a bottleneck.\n",
    "\n",
    "The encoder part of the network is used for encoding and sometimes even for data compression purposes although it is not very effective as compared to other general compression techniques like JPEG. Encoding is achieved by the encoder part of the network which has decreasing number of hidden units in each layer. Thus this part is forced to pick up only the most significant and representative features of the data. The second half of the network performs the Decoding function. This part has the increasing number of hidden units in each layer and thus tries to reconstruct the original input from the encoded data.\n",
    "\n",
    "Thus Auto-encoders are an unsupervised learning technique.\n",
    "\n",
    "Training of an Auto-encoder for data compression: For a data compression procedure, the most important aspect of the compression is the reliability of the reconstruction of the compressed data. This requirement dictates the structure of the Auto-encoder as a bottleneck.\n",
    "\n",
    "Step 1: Encoding the input data\n",
    "\n",
    "The Auto-encoder first tries to encode the data using the initialized weights and biases.\n",
    "\n",
    "Step 2: Decoding the input data\n",
    "\n",
    "The Auto-encoder tries to reconstruct the original input from the encoded data to test the reliability of the encoding.\n",
    "\n",
    "Step 3: Backpropagating the error\n",
    "\n",
    "After the reconstruction, the loss function is computed to determine the reliability of the encoding. The error generated is backpropagated.\n",
    "\n",
    "The above-described training process is reiterated several times until an acceptable level of reconstruction is reached.\n",
    "\n",
    "After the training process, only the encoder part of the Auto-encoder is retained to encode a similar type of data used in the training process.\n",
    "\n",
    "The different ways to constrain the network are:-\n",
    "\n",
    "Keep small Hidden Layers: If the size of each hidden layer is kept as small as possible, then the network will be forced to pick up only the representative features of the data thus encoding the data.\n",
    "Regularization: In this method, a loss term is added to the cost function which encourages the network to train in ways other than copying the input.\n",
    "Denoising: Another way of constraining the network is to add noise to the input and teaching the network how to remove the noise from the data.\n",
    "Tuning the Activation Functions: This method involves changing the activation functions of various nodes so that a majority of the nodes are dormant thus effectively reducing the size of the hidden layers.\n",
    "\n",
    "The different variations of Auto-encoders are:-\n",
    "\n",
    "Denoising Auto-encoder: This type of auto-encoder works on a partially corrupted input and trains to recover the original undistorted image. As mentioned above, this method is an effective way to constrain the network from simply copying the input.\n",
    "Sparse Auto-encoder: This type of auto-encoder typically contains more hidden units than the input but only a few are allowed to be active at once. This property is called the sparsity of the network. The sparsity of the network can be controlled by either manually zeroing the required hidden units, tuning the activation functions or by adding a loss term to the cost function.\n",
    "Variational Auto-encoder: This type of auto-encoder makes strong assumptions about the distribution of latent variables and uses the Stochastic Gradient Variational Bayes estimator in the training process. It assumes that the data is generated by a Directed Graphical Model and tries to learn an approximation to $q_{\\phi}(z|x)$ to the conditional property $q_{\\theta}(z|x)$ where $\\phi$ and $\\theta$ are the parameters of the encoder and the decoder respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoEncoder with TensorFlow 2.0\n",
    "\n",
    "This tutorial demonstrates how to generate images of handwritten digits using graph mode execution in TensorFlow 2.0 by training an Autoencoder. \n",
    "An AutoEncoder is a data compression and decompression algorithm implemented with Neural Networks and/or Convolutional Neural Networks. the data is compressed to a bottleneck that is of a lower dimension than the initial input. The decompression uses the intermediate representation to generate the same input image again. Let us code up a good AutoEncoder using TensorFlow 2.0 which is eager by default to understand the mechanism of this algorithm. AutoEncoders are considered a good pre-requisite for more advanced generative models such as GANs and CVAEs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "# Install TensorFlow 2.0 by using the following command\n",
    "# For CPU installation\n",
    "# pip install -q tensorflow == 2.0\n",
    "# For GPU installation (CUDA and CuDNN must be available)\n",
    "# pip install -q tensorflow-gpu == 2.0\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After confirming the appropriate TF download, import the other dependencies for data augmentation and define custom functions as shown below. The standard scaler scales the data by transforming the columns. The get_random_block_from_data function is useful when using tf.GradientTape to perform AutoDiff (Automatic Differentiation) to get the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.preprocessing as prep\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "def standard_scale(X_train, X_test):\n",
    "\tpreprocessor = prep.StandardScaler().fit(X_train)\n",
    "\tX_train = preprocessor.transform(X_train)\n",
    "\tX_test = preprocessor.transform(X_test)\n",
    "\treturn X_train, X_test\n",
    "\n",
    "def get_random_block_from_data(data, batch_size):\n",
    "\tstart_index = np.random.randint(0, len(data) - batch_size)\n",
    "\treturn data[start_index:(start_index + batch_size)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoEncoders may have a lossy intermediate representation also known as a compressed representation. This dimensionality reduction is useful in a multitude of use cases where lossless image data compression exists. Thus we can say that the encoder part of the AutoEncoder encodes a dense representation of the data. Here we will use TensorFlow Subclassing API to define custom layers for the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "\t'''Encodes a digit from the MNIST dataset'''\n",
    "\t\n",
    "\tdef __init__(self,\n",
    "\t\t\t\tn_dims,\n",
    "\t\t\t\tname ='encoder',\n",
    "\t\t\t\t**kwargs):\n",
    "\t\tsuper(Encoder, self).__init__(name = name, **kwargs)\n",
    "\t\tself.n_dims = n_dims\n",
    "\t\tself.n_layers = 1\n",
    "\t\tself.encode_layer = layers.Dense(n_dims, activation ='relu')\n",
    "\t\t\n",
    "\t@tf.function\t\n",
    "\tdef call(self, inputs):\n",
    "\t\treturn self.encode_layer(inputs)\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "\t'''Decodes a digit from the MNIST dataset'''\n",
    "\n",
    "\tdef __init__(self,\n",
    "\t\t\t\tn_dims,\n",
    "\t\t\t\tname ='decoder',\n",
    "\t\t\t\t**kwargs):\n",
    "\t\tsuper(Decoder, self).__init__(name = name, **kwargs)\n",
    "\t\tself.n_dims = n_dims\n",
    "\t\tself.n_layers = len(n_dims)\n",
    "\t\tself.decode_middle = layers.Dense(n_dims[0], activation ='relu')\n",
    "\t\tself.recon_layer = layers.Dense(n_dims[1], activation ='sigmoid')\n",
    "\t\t\n",
    "\t@tf.function\t\n",
    "\tdef call(self, inputs):\n",
    "\t\tx = self.decode_middle(inputs)\n",
    "\t\treturn self.recon_layer(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then extend tf.keras.Model to define a custom model that utilizes our previously defined custom layers to form the AutoEncoder model. The call function is overridden which is the forward passwhen the data is made available to the model object. Notice the @tf.function function decorator. It ensures that the function execution occurs in a graph which speeds up our execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3136)              9408      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 65,089\n",
      "Trainable params: 65,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class Autoencoder(tf.keras.Model):\n",
    "\t'''Vanilla Autoencoder for MNIST digits'''\n",
    "\t\n",
    "\tdef __init__(self,\n",
    "\t\t\t\tn_dims =[200, 392, 784],\n",
    "\t\t\t\tname ='autoencoder',\n",
    "\t\t\t\t**kwargs):\n",
    "\t\tsuper(Autoencoder, self).__init__(name = name, **kwargs)\n",
    "\t\tself.n_dims = n_dims\n",
    "\t\tself.encoder = Encoder(n_dims[0])\n",
    "\t\tself.decoder = Decoder([n_dims[1], n_dims[2]])\n",
    "\t\t\n",
    "\t@tf.function\t\n",
    "\tdef call(self, inputs):\n",
    "\t\tx = self.encoder(inputs)\n",
    "\t\treturn self.decoder(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block prepares the dataset and gets the data ready to be fed into the pre-processing pipeline of functions before training the AutoEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(X_train, _), (X_test, _) = mnist.load_data()\n",
    "X_train = tf.cast(np.reshape(\n",
    "\t\tX_train, (X_train.shape[0],\n",
    "\t\t\t\tX_train.shape[1] * X_train.shape[2])), tf.float64)\n",
    "X_test = tf.cast(\n",
    "\t\tnp.reshape(X_test,\n",
    "\t\t\t\t(X_test.shape[0],\n",
    "\t\t\t\t\tX_test.shape[1] * X_test.shape[2])), tf.float64)\n",
    "\n",
    "X_train, X_test = standard_scale(X_train, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is TensorFlow best practice to use tf.data.Dataset to get tensor slices with a shuffled batch quickly from the dataset for training. The following code block demonstrates teh use of tf.data and also defines the hyperparameters for training the AutoEncoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094/1094 [==============================] - 69s 63ms/step - loss: 276.8902 - reconstruction_loss: 273.5193 - kl_loss: 3.3709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c800cbdc70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "\t\tX_train).batch(128).shuffle(buffer_size = 1024)\n",
    "test_data = tf.data.Dataset.from_tensor_slices(\n",
    "\t\tX_test).batch(128).shuffle(buffer_size = 512)\n",
    "\n",
    "n_samples = int(len(X_train) + len(X_test))\n",
    "training_epochs = 20\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate = 0.01)\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have completed every pre-requisite to train our AutoEncoder model! All we have left to do is to define an AutoEncoder object and compile the model with the optimizer and loss before calling model.train on it for the hyperparameters defined above. Voila! You can see the loss reducing and the AutoEncoder improving its performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = Autoencoder([200, 392, 784])\n",
    "ae.compile(optimizer = tf.optimizers.Adam(0.01),\n",
    "\t\tloss ='categorical_crossentropy')\n",
    "ae.fit(X_train, X_train, batch_size = 64, epochs = 5)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c57540b4cf4ee0a0ebe1ff89debbd45a00df8f46dd0a264af7f7b9499529b598"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('nn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
